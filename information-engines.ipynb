{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Engines:  Intelligence and Thermodynamics\n",
    "\n",
    "## Neil D. Lawrence\n",
    "\n",
    "## 9th January 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\phaseVariables}{\\boldsymbol{\\Gamma}}\n",
    "\\newcommand{\\stateVariables}{\\mathbf{x}}\n",
    "\\newcommand{\\nullVariables}{\\mathbf{x}_0}\n",
    "\\newcommand{\\domainVariables}{\\mathbf{x}_1}\n",
    "\\newcommand{\\dataVariables}{\\mathbf{y}}\n",
    "\\newcommand{\\parameterVector}{\\mathbf{w}}\n",
    "\\renewcommand{\\phaseVariables}{\\Gamma}\n",
    "\\renewcommand{\\stateVariables}{X}\n",
    "\\renewcommand{\\nullVariables}{X_0}\n",
    "\\renewcommand{\\domainVariables}{X_1}\n",
    "\\renewcommand{\\dataVariables}{Y}\n",
    "\\renewcommand{\\parameterVector}{W}\n",
    "\\newcommand{\\expDist}[2]{\\left\\langle #1 \\right\\rangle_{#2}}\n",
    "\\newcommand{\\trueProb}{\\mathbb{P}}\n",
    "\\newcommand{\\physicsProb}{p}\n",
    "\\newcommand{\\approxProb}{q}\n",
    "\\newcommand{\\statsProb}{\\pi}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ashby's concept of \"variety\", the requisite law of vareity: https://en.wikipedia.org/wiki/Variety_(cybernetics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Helmholtz free energy was derived by Hermann von Helmholtz in the study of electrochemistry. \n",
    "\n",
    "In a thermodynamic system, the state space is referred to as the phase space[^1] \n",
    "\n",
    "[^1]: Here we are referring to the microstates, the macrostates would be the sufficient statistics of the microstates. \n",
    "\n",
    "Boltzmann's distribution gives us the probability distribution associated with that state space. In a continuous system, we write down the the *Hamiltonian*, the energy associated with the phase space.\n",
    "$$\n",
    "\\mathbb{P}(\\phaseVariables) = \\frac{1}{Z_\\phaseVariables} \\exp(-\\beta E(\\phaseVariables))\n",
    "$$\n",
    "where $E(\\phaseVariables)$ is the energy of the system in microstate $\\phaseVariables$. \n",
    "\n",
    "The partition function is given by \n",
    "$$\n",
    "Z_\\phaseVariables = \\sum_\\phaseVariables \\exp(-\\beta E(\\phaseVariables)),\n",
    "$$\n",
    "where the $Z$ comes from the German for \"sum over states\" or *Zustandssumme*, reflecting Boltzmann's Austrian heritage. In the case of continuous systems we have,\n",
    "$$\n",
    "Z_\\phaseVariables = \\frac{1}{h^3}\\int \\exp(-\\beta E(\\phaseVariables)) \\text{d} \\phaseVariables\n",
    "$$\n",
    "and $E(\\phaseVariables)$ is the Hamiltonian of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Energy\n",
    "\n",
    "The total energy, $U_\\phaseVariables$ is defined as the expected energy,\n",
    "$$\n",
    "U_\\phaseVariables = \\expDist{E(\\phaseVariables)}{\\trueProb(\\phaseVariables)}\n",
    "$$\n",
    "which can be decomposed using the definition of $\\trueProb(\\phaseVariables)$ as\n",
    "$$\n",
    "U_\\phaseVariables = A_\\phaseVariables + TS_\\phaseVariables\n",
    "$$\n",
    "where\n",
    "$$\n",
    "A_\\phaseVariables = - \\frac{1}{\\beta}\\log Z_\\phaseVariables\n",
    "$$\n",
    "is the *Helmholtz free energy* and \n",
    "$$\n",
    "S_\\phaseVariables = -k_B \\expDist{\\log \\trueProb(\\phaseVariables)}{\\trueProb(\\phaseVariables)}\n",
    "$$\n",
    "is the entropy of the system and $T$ is the temperature. \n",
    "\n",
    "This equation expresses a fundamental decomposition of the total energy into the available energy, $A_\\phaseVariables$ and energy that is not available, $TS_\\phaseVariables$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By our definition of intelligence, the aim is to use information to achieve a goal with less resource. Here we'll interpret that as increasing available energy. First, we're split the phase space into a the unobserved part, and an observed part $\\phaseVariables = \\{\\stateVariables, \\dataVariables\\}$, this allows us to write\n",
    "$$\n",
    "U_{\\stateVariables,\\dataVariables} = A_{\\stateVariables,\\dataVariables} + TS_{\\stateVariables,\\dataVariables}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "A_{\\stateVariables,\\dataVariables} = - \\frac{1}{\\beta}\\log Z_{\\stateVariables,\\dataVariables}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "S_{\\stateVariables,\\dataVariables} = -k_B \\expDist{\\log \\trueProb({\\stateVariables,\\dataVariables})}{\\trueProb({\\stateVariables,\\dataVariables})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information to Available Energy\n",
    "\n",
    "Making an observation in this system is equivalent to conditioning on $\\dataVariables$ for the total energy which we write as\n",
    "$$\n",
    "U_{\\stateVariables|\\dataVariables} = \\expDist{E({\\stateVariables|\\dataVariables})}{\\trueProb({\\stateVariables|\\dataVariables})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "\\mathbb{P}({\\stateVariables|\\dataVariables}) = \\frac{1}{Z_{\\stateVariables|\\dataVariables}} \\exp\\left(-\\beta E(\\dataVariables|\\stateVariables) -\\beta E(\\stateVariables)\\right)\n",
    "$$\n",
    "where we have decomposed $E(\\stateVariables, \\dataVariables)$ into two parts, one which represents the interaction between our measurements and the state, $E(\\dataVariables|\\stateVariables)$ and $E(\\stateVariables)$ represents energy terms where there is no interaction and\n",
    "$$\n",
    "Z_{\\stateVariables|\\dataVariables} = \\int \\exp\\left(-\\beta E(\\dataVariables|\\stateVariables) -\\beta E(\\stateVariables)\\right) \\text{d}\\stateVariables\n",
    "$$\n",
    "The new total energy, conditioning on the measurements is\n",
    "$$\n",
    "U_{\\stateVariables|\\dataVariables} = A_{\\stateVariables|\\dataVariables} + TS_{\\stateVariables|\\dataVariables}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "A_{\\stateVariables|\\dataVariables} = - \\frac{1}{\\beta}\\log Z_{\\stateVariables|\\dataVariables}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "S_{\\stateVariables|\\dataVariables} = -k_B \\expDist{\\log \\trueProb({\\stateVariables|\\dataVariables})}{\\trueProb({\\stateVariables|\\dataVariables})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine how this changes the free energy, the energy gain through observation is,\n",
    "$$\n",
    "\\begin{align*}\n",
    "A_{\\stateVariables|\\dataVariables} - A_{\\stateVariables,\\dataVariables} = & -\\frac{1}{\\beta} \\log \\frac{Z_{\\stateVariables|\\dataVariables}}{Z_{\\stateVariables,\\dataVariables}}\\\\\n",
    "& -T k_B \\log \\trueProb(\\dataVariables)\n",
    "\\end{align*}\n",
    "$$\n",
    "which is the information gained through the observation, $\\dataVariables$.\n",
    "\n",
    "This shows the relationship between measurement and energy. By measuring the system we can gain free energy. The less likely the measurements, the more energy we gain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b5/Figchannel2017ab.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Available Energy\n",
    "\n",
    "Unfortunately, we do not know the true distribution, and cannot compute how much energy we've gained. The full model, $\\trueProb(\\stateVariables, \\dataVariables)$, is not available to us. Indeed, the number of states across the universe is so many, that even if we knew all the physics, we couldn't write down the model of everything. \n",
    "\n",
    "The available energy is\n",
    "$$\\begin{align*}\n",
    "A_{\\stateVariables|\\dataVariables} = & -\\frac{1}{\\beta} \\log Z_{\\stateVariables|\\dataVariables} \\\\\n",
    "= & -\\frac{1}{\\beta} \\log \\int \\exp\\left(-\\beta E(\\dataVariables|\\stateVariables) -\\beta E(\\stateVariables)\\right) \\text{d}\\stateVariables\n",
    "\\end{align*}\n",
    "$$\n",
    "which we can rewrite as\n",
    "$$\n",
    "A_{\\stateVariables|\\dataVariables} =  -\\frac{1}{\\beta}\\expDist{\\log \\frac{\\exp\\left(-\\beta E(\\dataVariables|\\stateVariables) -\\beta E(\\stateVariables)\\right)}{\\physicsProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\stateVariables|\\dataVariables) } - \\frac{1}{\\beta}\\expDist{\\log \\frac{\\physicsProb(\\stateVariables|\\dataVariables)}{\\trueProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\stateVariables|\\dataVariables) }\n",
    "$$\n",
    "or alternatively\n",
    "$$\n",
    "A_{\\stateVariables|\\dataVariables} = U^\\prime_{\\stateVariables|\\dataVariables} - TS^\\prime_{\\stateVariables|\\dataVariables} - TM_{\\stateVariables|\\dataVariables}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "U^\\prime_{\\stateVariables|\\dataVariables} = \\expDist{E(\\dataVariables|\\stateVariables)}{\\physicsProb(\\stateVariables|\\dataVariables) } \n",
    "$$\n",
    "and\n",
    "$$\n",
    "S^\\prime_{\\stateVariables|\\dataVariables}=k_B\\expDist{\\log\\physicsProb(\\stateVariables|\\dataVariables) }{\\physicsProb(\\stateVariables|\\dataVariables) }\n",
    "$$\n",
    "where $M_{\\stateVariables|\\dataVariables}$ is the model misspecification,\n",
    "$$\n",
    "M_{\\stateVariables|\\dataVariables} = k_B \\expDist{\\log \\frac{\\physicsProb(\\stateVariables| \\dataVariables)}{\\trueProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\stateVariables|\\dataVariables) },\n",
    "$$\n",
    "which is recognised as the Kullback-Leibler diverence (information theoretic term) or the *relative entropy* between the true model and our approximation. \n",
    "\n",
    "We also define\n",
    "$$\n",
    "A^\\prime_{\\stateVariables|\\dataVariables} = U^\\prime_{\\stateVariables|\\dataVariables} - TS^\\prime_{\\stateVariables|\\dataVariables}\n",
    "$$\n",
    "so we have\n",
    "$$\n",
    "A^\\prime_{\\stateVariables|\\dataVariables} = A_{\\stateVariables|\\dataVariables} + TM_{\\stateVariables|\\dataVariables}.\n",
    "$$\n",
    "showing that $A^\\prime_{\\stateVariables|\\dataVariables}$ is a lower bound on the true free energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Energy Gain\n",
    "\n",
    "Our interest is not the true free energy, but the energy we gain through making the observation. This is denoted\n",
    "$$\\begin{align*}\n",
    "A_{\\stateVariables|\\dataVariables} - A_{\\stateVariables,\\dataVariables} = & -\\frac{1}{\\beta} \\log \\int \\exp\\left(-\\beta \\hat{E}(\\dataVariables|\\stateVariables) -\\beta E(\\stateVariables)\\right) \\text{d}\\stateVariables \\\\\n",
    "& +\\frac{1}{\\beta} \\log \\int \\exp\\left(-\\beta E(\\dataVariables|\\stateVariables) -\\beta E(\\stateVariables)\\right) \\text{d}\\stateVariables\\text{d}\\dataVariables\n",
    "\\end{align*}\n",
    "$$\n",
    "where \n",
    "$\\hat{E}(\\dataVariables|\\stateVariables)\\geq E(\\dataVariables|\\stateVariables)$ also includes the additional energy cost of conditioning on $\\dataVariables$. \n",
    "\n",
    "$$\\begin{align*}\n",
    "A_{\\stateVariables|\\dataVariables} - A_{\\stateVariables,\\dataVariables} =  & -\\frac{1}{\\beta}\\expDist{\\log \\frac{\\exp\\left(-\\beta \\hat{E}(\\dataVariables|\\stateVariables) -\\beta E(\\stateVariables)\\right)}{\\physicsProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\stateVariables|\\dataVariables) } - \\frac{1}{\\beta}\\expDist{\\log \\frac{\\physicsProb(\\stateVariables|\\dataVariables)}{\\trueProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\stateVariables|\\dataVariables) }\\\\\n",
    "& +\\frac{1}{\\beta}\\expDist{\\log \\frac{\\exp\\left(-\\beta E(\\dataVariables|\\stateVariables) -\\beta E(\\stateVariables)\\right)}{\\physicsProb(\\stateVariables,\\dataVariables) }}{\\physicsProb(\\stateVariables,\\dataVariables) } + \\frac{1}{\\beta}\\expDist{\\log \\frac{\\physicsProb(\\stateVariables,\\dataVariables)}{\\trueProb(\\stateVariables,\\dataVariables) }}{\\physicsProb(\\stateVariables,\\dataVariables) }\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "A_{\\stateVariables|\\dataVariables} - A_{\\stateVariables,\\dataVariables} =  & -\\frac{1}{\\beta}\\expDist{\\log \\frac{\\exp\\left(-\\beta \\hat{E}(\\dataVariables|\\stateVariables) -\\beta E(\\stateVariables)\\right)}{\\physicsProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\stateVariables|\\dataVariables) } - \\frac{1}{\\beta}\\expDist{\\log \\frac{\\physicsProb(\\stateVariables|\\dataVariables)}{\\trueProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\stateVariables|\\dataVariables) }\\\\\n",
    "& +\\frac{1}{\\beta}\\expDist{\\log \\frac{\\exp\\left(-\\beta E(\\dataVariables|\\stateVariables) -\\beta E(\\stateVariables)\\right)}{\\physicsProb(\\stateVariables,\\dataVariables) }}{\\physicsProb(\\stateVariables,\\dataVariables) } + \\frac{1}{\\beta}\\expDist{\\log \\frac{\\physicsProb(\\stateVariables|\\dataVariables)}{\\trueProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\stateVariables,\\dataVariables) } + \\frac{1}{\\beta}\\expDist{\\log \\frac{\\physicsProb(\\dataVariables)}{\\trueProb(\\dataVariables) }}{\\physicsProb(\\dataVariables) }\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "A_{\\stateVariables|\\dataVariables} - A_{\\stateVariables,\\dataVariables} =  & U^\\prime_{\\stateVariables|\\dataVariables} - U^\\prime_{\\stateVariables,\\dataVariables} + TS^\\prime_\\dataVariables \\\\\n",
    "& - \\frac{1}{\\beta}\\expDist{\\log \\frac{\\physicsProb(\\stateVariables|\\dataVariables)}{\\trueProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\stateVariables|\\dataVariables) } \\\\\n",
    " & + \\expDist{\\frac{1}{\\beta}\\expDist{\\log \\frac{\\physicsProb(\\stateVariables|\\dataVariables)}{\\trueProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\stateVariables|\\dataVariables) }}{\\physicsProb(\\dataVariables)}\\\\\n",
    "&  + \\frac{1}{\\beta}\\expDist{\\log \\frac{\\physicsProb(\\dataVariables)}{\\trueProb(\\dataVariables) }}{\\physicsProb(\\dataVariables) }\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "A_{\\stateVariables|\\dataVariables} - A_{\\stateVariables,\\dataVariables} =  & U^\\prime_{\\stateVariables|\\dataVariables} - U^\\prime_{\\stateVariables,\\dataVariables} + TS^\\prime_\\dataVariables - T\\left(M_{\\stateVariables|\\dataVariables} - \\expDist{M_{\\stateVariables|\\dataVariables}}{\\physicsProb(\\dataVariables)} - M_{\\dataVariables}\\right)\n",
    "\\end{align*}$$\n",
    "where\n",
    "$$\n",
    "S^\\prime_{\\dataVariables}=-k_B\\expDist{\\log\\physicsProb(\\dataVariables) }{\\physicsProb(\\dataVariables) }\n",
    "$$\n",
    "and $M_{\\dataVariables}$ is the model misspecification,\n",
    "$$\n",
    "M_{\\dataVariables} = k_B \\expDist{\\log \\frac{\\physicsProb( \\dataVariables)}{\\trueProb(\\dataVariables) }}{\\physicsProb(\\dataVariables) },\n",
    "$$\n",
    "\n",
    "If we look at the expected change under all data sets we get\n",
    "$$\n",
    "\\expDist{A_{\\stateVariables|\\dataVariables} - A_{\\stateVariables,\\dataVariables}}{\\trueProb(\\dataVariables)} = -\\expDist{E(\\dataVariables)}{\\trueProb(\\dataVariables)} + TS^\\prime_\\dataVariables + TM_{\\dataVariables}  - T\\expDist{\\left(M_{\\stateVariables|\\dataVariables} - \\expDist{M_{\\stateVariables|\\dataVariables}}{\\physicsProb(\\dataVariables)}\\right)}{\\trueProb(\\dataVariables)}\n",
    "$$\n",
    "where $E(\\dataVariables)$ is the energy cost of measuring $\\dataVariables$ and \n",
    "$$\n",
    "\\expDist{\\left(M_{\\stateVariables|\\dataVariables} - \\expDist{M_{\\stateVariables|\\dataVariables}}{\\physicsProb(\\dataVariables)}\\right)}{\\trueProb(\\dataVariables)}\n",
    "$$\n",
    "\n",
    "\n",
    "So we have\n",
    "$$\n",
    "\\expDist{\\Delta A^\\prime_{\\stateVariables|\\dataVariables}}{\\trueProb(\\dataVariables)} = -\\expDist{E(\\dataVariables)}{\\trueProb(\\dataVariables)} + TS^\\prime_\\dataVariables \n",
    "$$\n",
    "where \n",
    "$$\n",
    "E(\\dataVariables) = \\expDist{\\hat{E}(\\dataVariables|\\stateVariables)}{\\physicsProb(\\stateVariables|\\dataVariables)} - \\expDist{E(\\dataVariables|\\stateVariables)}{\\physicsProb(\\stateVariables,\\dataVariables)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Space and Domain Space\n",
    "\n",
    "One challenge is dealing with the number of variables in the world. Our first step will be to partition the state space into a *null space*, $\\nullVariables$ and a domain space, $\\domainVariables$, $\\stateVariables = \\{\\nullVariables, \\domainVariables\\}$. The domain space contains variables of interest to our problem, and the null space contains variables that are not directly of interest. In particular the null space contains variables that we believe are only weakly influenced by our measurements. The next step is to assume that our approximating density factorises across these spaces,\n",
    "$$\n",
    "\\physicsProb(\\nullVariables, \\domainVariables | \\dataVariables) = \\physicsProb(\\nullVariables|\\dataVariables) \\physicsProb(\\domainVariables | \\dataVariables).\n",
    "$$\n",
    "\n",
    "If this is the only assumption we make about our approximation to the real world, then we can compute the optimum approximations by minimizing the model mispecification. \n",
    "$$\n",
    "\\physicsProb(\\domainVariables|\\dataVariables) = \\frac{\\exp\\left(-\\beta E^\\prime(\\dataVariables |\\domainVariables) - \\beta E^\\prime(\\domainVariables)\\right)}{Z^\\prime_{\\domainVariables | \\dataVariables}} \n",
    "$$\n",
    "where\n",
    "$$\\begin{align*}\n",
    "E^\\prime(\\dataVariables |\\domainVariables) = & \\expDist{E(\\dataVariables |\\stateVariables)}{\\physicsProb(\\nullVariables|\\dataVariables)} \\\\\n",
    "E^\\prime(\\domainVariables) = & \\expDist{E(\\stateVariables)}{\\physicsProb(\\nullVariables|\\dataVariables)} \\\\\n",
    "Z^\\prime_{\\domainVariables | \\dataVariables} = & \\int \\exp\\left(-\\beta E^\\prime(\\dataVariables |\\domainVariables) - \\beta E^\\prime(\\domainVariables)\\right) \\text{d}\\domainVariables. \n",
    "\\end{align*}\n",
    "$$\n",
    "Similary,\n",
    "$$\n",
    "\\physicsProb(\\nullVariables|\\dataVariables) = \\frac{\\exp\\left(-\\beta E^\\prime(\\dataVariables |\\nullVariables) - \\beta E^\\prime(\\nullVariables)\\right)}{Z^\\prime_{\\nullVariables}} \n",
    "$$\n",
    "where\n",
    "$$\\begin{align*}\n",
    "E^\\prime(\\dataVariables |\\nullVariables) = & \\expDist{E(\\dataVariables |\\stateVariables)}{\\physicsProb(\\domainVariables|\\dataVariables)} \\\\\n",
    "E^\\prime(\\nullVariables) = & \\expDist{E(\\stateVariables)}{\\physicsProb(\\domainVariables|\\dataVariables)} \\\\\n",
    "Z^\\prime_{\\nullVariables | \\dataVariables} = & \\int \\exp\\left(-\\beta E^\\prime(\\dataVariables |\\nullVariables) - \\beta E^\\prime(\\nullVariables)\\right) \\text{d}\\nullVariables. \n",
    "\\end{align*}\n",
    "$$\n",
    "Note that these two distributions are interdependent. The optimal form of $\\physicsProb(\\nullVariables|\\dataVariables)$ is dependent on the form of $\\physicsProb(\\domainVariables|\\dataVariables)$ and vice versa.\n",
    "\n",
    "The two distributions are symmetric. Our assumption is that the domain variables are more influenced by the mearsurements, $\\dataVariables$, than the null space. So we will focus on the domain variables. But due to the symmetry the derivation that follows would apply equally to the null space variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apparent Free Energy\n",
    "\n",
    "Dropping the model misspecification we now have an upper bound on the free energy,\n",
    "$$\n",
    "A_{\\stateVariables|\\dataVariables} \\leq U^\\prime_{\\stateVariables|\\dataVariables} - TS^\\prime_{\\stateVariables|\\dataVariables} \n",
    "$$\n",
    "or alternatively, we can define\n",
    "$$\n",
    "A^\\prime_{\\stateVariables|\\dataVariables} = U^\\prime_{\\stateVariables|\\dataVariables} - TS^\\prime_{\\stateVariables|\\dataVariables}. \n",
    "$$\n",
    "which we call the *apparent free energy*. The apparent free energy is an upper bound on the true free energy. Our factorisation into domain and null space reflects the fact that we think we view $\\physicsProb(\\domainVariables | \\dataVariables)$ as a model emerging from a domain expert. \n",
    "\n",
    "By our definitions above, the apparent total energy can be written as\n",
    "$$\n",
    "U^\\prime_{\\stateVariables|\\dataVariables} = \\expDist{E^\\prime(\\dataVariables |\\domainVariables)}{\\physicsProb(\\domainVariables| \\dataVariables)} + \\expDist{E^\\prime(\\domainVariables)}{\\physicsProb(\\domainVariables| \\dataVariables)} = U^\\prime_{\\domainVariables|\\dataVariables} = U^\\prime_{\\nullVariables|\\dataVariables}\n",
    "$$\n",
    "and our independence assumption means that\n",
    "$$\n",
    "S^\\prime_{\\stateVariables|\\dataVariables} = S^\\prime_{\\domainVariables|\\dataVariables} + S^\\prime_{\\nullVariables|\\dataVariables}.\n",
    "$$\n",
    "We now can define \n",
    "$$\n",
    "A^\\prime_{\\domainVariables|\\dataVariables} = U^\\prime_{\\domainVariables|\\dataVariables} - TS^\\prime_{\\domainVariables|\\dataVariables} = -\\frac{1}{\\beta} \\log Z^\\prime_{\\domainVariables|\\dataVariables}\n",
    "$$\n",
    "and can see that the apparent free energy is\n",
    "$$\n",
    "A^\\prime_{\\stateVariables|\\dataVariables} = A^\\prime_{\\domainVariables|\\dataVariables} - TS^\\prime_{\\nullVariables|\\dataVariables}.\n",
    "$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterised Model\n",
    "\n",
    "The next step is to include *parameters* with the model. To do this we first separate a new set of auxiliary variables from the domain variables, $\\domainVariables \\rightarrow \\{\\domainVariables, \\parameterVector\\}$introduce a new distribution, \n",
    "$$\n",
    "\\approxProb(\\domainVariables, \\parameterVector) = \\physicsProb(\\domainVariables | \\parameterVector)\\approxProb(\\parameterVector)\n",
    "$$\n",
    "$$\\begin{align*}\n",
    "A^\\prime_{\\domainVariables|\\parameterVector} = & -\\frac{1}{\\beta} \\log \\int \\exp\\left(-\\beta E^\\prime(\\dataVariables |\\domainVariables, \\parameterVector) - \\beta E^\\prime(\\domainVariables, \\parameterVector)\\right) \\text{d}\\domainVariables \\text{d}\\parameterVector\\\\\n",
    "= & -\\frac{1}{\\beta} \\expDist{\\log\\frac{\\exp\\left(-\\beta E^\\prime(\\dataVariables |\\domainVariables, \\parameterVector) - \\beta E^\\prime(\\domainVariables, \\parameterVector)\\right)}{\\approxProb(\\domainVariables, \\parameterVector)}}{\\approxProb(\\domainVariables, \\parameterVector)} - \\frac{1}{\\beta} \\expDist{\\log\\frac{\\approxProb(\\domainVariables , \\parameterVector)}{\\physicsProb(\\domainVariables, \\parameterVector | \\dataVariables)}}{\\approxProb(\\domainVariables, \\parameterVector)} \\\\\n",
    "= & -\\frac{1}{\\beta} \\expDist{\\log\\frac{\\exp\\left(-\\beta \\expDist{E^\\prime(\\dataVariables |\\domainVariables, \\parameterVector) - \\beta E^\\prime(\\domainVariables, \\parameterVector)\\right)}{\\physicsProb(\\domainVariables | \\parameterVector)}}{\\approxProb(\\parameterVector)}}{\\approxProb(\\parameterVector)} + \\frac{1}{\\beta} \\expDist{\\log \\physicsProb(\\domainVariables | \\parameterVector)}{\\approxProb(\\domainVariables, \\parameterVector)} - \\frac{1}{\\beta} \\expDist{\\log\\frac{\\approxProb(\\domainVariables , \\parameterVector)}{\\physicsProb(\\domainVariables, \\parameterVector | \\dataVariables)}}{\\approxProb(\\domainVariables, \\parameterVector)} \\\\\n",
    "= & -\\frac{1}{\\beta} \\expDist{\\log\\frac{\\exp\\left(-\\beta E^{\\prime\\prime}(\\dataVariables |\\parameterVector) - \\beta E^{\\prime\\prime}(\\parameterVector)\\right)}{\\approxProb(\\parameterVector)}}{\\approxProb(\\parameterVector)} + \\frac{1}{\\beta} \\expDist{\\expDist{\\log \\physicsProb(\\domainVariables | \\parameterVector)}{\\physicsProb(\\domainVariables | \\parameterVector)}}{\\approxProb(\\parameterVector)} - \\frac{1}{\\beta} \\expDist{\\expDist{\\log\\frac{\\physicsProb(\\domainVariables | \\parameterVector)}{\\physicsProb(\\domainVariables| \\parameterVector , \\dataVariables)}}{\\physicsProb(\\domainVariables | \\parameterVector)}}{\\approxProb(\\parameterVector)} - \\frac{1}{\\beta}\\expDist{\\log\\frac{\\approxProb(\\parameterVector)}{\\physicsProb(\\parameterVector|\\dataVariables)}}{\\approxProb(\\parameterVector)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we define \n",
    "$$\n",
    "\\statsProb(\\dataVariables | \\parameterVector) = \\frac{\\exp\\left(-\\beta E^{\\prime\\prime}(\\dataVariables |\\parameterVector)\\right)}{Z^{\\prime\\prime}_{\\dataVariables | \\parameterVector}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\statsProb(\\parameterVector) = \\frac{\\exp\\left(-\\beta E^{\\prime\\prime}(\\parameterVector)\\right)}{Z^{\\prime\\prime}_{\\parameterVector}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "Z^{\\prime\\prime}_{\\dataVariables | \\parameterVector} = \\int \\exp\\left(-\\beta E^{\\prime\\prime}(\\dataVariables |\\parameterVector)\\right) \\text{d}\\dataVariables\n",
    "$$\n",
    "and\n",
    "$$\n",
    "Z^{\\prime\\prime}_{\\parameterVector} = \\int \\exp\\left(-\\beta E^{\\prime\\prime}(\\parameterVector)\\right) \\text{d}\\parameterVector\n",
    "$$\n",
    "and\n",
    "$$\n",
    "Z^{\\prime\\prime}_{\\parameterVector|\\dataVariables} = \\int \\exp\\left(-\\beta E^{\\prime\\prime}(\\dataVariables |\\parameterVector)\\right)  + \\exp\\left(-\\beta E^{\\prime\\prime}(\\parameterVector)\\right) \\text{d}\\parameterVector\n",
    "$$\n",
    "and we set\n",
    "$$\n",
    "\\approxProb(\\parameterVector) = \\statsProb(\\parameterVector |\\dataVariables)\n",
    "$$\n",
    "then we have\n",
    "$$\n",
    "A^\\prime_{\\domainVariables|\\parameterVector} = -\\frac{1}{\\beta} \\log  Z^{\\prime\\prime}_{\\parameterVector|\\dataVariables} - T\\expDist{S^\\prime_{\\domainVariables | \\parameterVector}}{\\statsProb(\\parameterVector | \\dataVariables)} - T\\expDist{B_{\\domainVariables | \\parameterVector}}{\\statsProb(\\parameterVector | \\dataVariables)}  - \\frac{1}{\\beta}\\expDist{\\log\\frac{\\statsProb(\\parameterVector|\\dataVariables)}{\\physicsProb(\\parameterVector|\\dataVariables)}}{\\statsProb(\\parameterVector|\\dataVariables)}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "B^\\prime_{\\domainVariables | \\parameterVector} = k_B \\expDist{\\log\\frac{\\physicsProb(\\domainVariables | \\parameterVector)}{\\physicsProb(\\domainVariables| \\parameterVector , \\dataVariables)}}{\\physicsProb(\\domainVariables | \\parameterVector)}\n",
    "$$\n",
    "so overall we have\n",
    "$$\n",
    "A_{\\stateVariables|\\dataVariables} = -\\frac{1}{\\beta} \\log  Z^{\\prime\\prime}_{\\parameterVector|\\dataVariables} - T\\expDist{S^\\prime_{\\domainVariables | \\parameterVector}}{\\statsProb(\\parameterVector | \\dataVariables)} - T\\expDist{B^\\prime_{\\domainVariables | \\parameterVector}}{\\statsProb(\\parameterVector | \\dataVariables)}  - TM^\\prime_{\\parameterVector|\\dataVariables} - TS^\\prime_{\\nullVariables|\\dataVariables}- TM_{\\stateVariables|\\dataVariables}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "M^\\prime_{\\stateVariables|\\dataVariables} =  k_B\\expDist{\\log\\frac{\\statsProb(\\parameterVector|\\dataVariables)}{\\physicsProb(\\parameterVector|\\dataVariables)}}{\\statsProb(\\parameterVector|\\dataVariables)}\n",
    "$$\n",
    "Collecting terms\n",
    "$$\n",
    "A_{\\stateVariables|\\dataVariables} = A^{\\prime\\prime}_{\\parameterVector|\\dataVariables} - T\\left[\\expDist{S^\\prime_{\\domainVariables | \\parameterVector}}{\\statsProb(\\parameterVector | \\dataVariables)} + S^\\prime_{\\nullVariables|\\dataVariables}\\right]   - T\\left[M^\\prime_{\\parameterVector|\\dataVariables} + M_{\\stateVariables|\\dataVariables} + \\expDist{B^\\prime_{\\domainVariables | \\parameterVector}}{\\statsProb(\\parameterVector | \\dataVariables)}\\right]\n",
    "$$\n",
    "Or\n",
    "$$\n",
    "A^{\\prime\\prime}_{\\parameterVector|\\dataVariables} = A_{\\stateVariables|\\dataVariables} + T\\left[\\expDist{S^\\prime_{\\domainVariables | \\parameterVector}}{\\statsProb(\\parameterVector | \\dataVariables)} + S^\\prime_{\\nullVariables|\\dataVariables}\\right]  + T\\left[M^\\prime_{\\parameterVector|\\dataVariables} + M_{\\stateVariables|\\dataVariables} + \\expDist{B^\\prime_{\\domainVariables | \\parameterVector}}{\\statsProb(\\parameterVector | \\dataVariables)}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speculative on Naming\n",
    "\n",
    "Spilt the statistical model mismatch into two terms that represent how 'correct' and how 'consistent' the statistical model is. \n",
    "\n",
    "Correctness  represents the ability of the model to reconstruct the information about domain variables,  $\\domainVariables$, that is provided by the data, $\\dataVariables$, through the parameters, $\\parameterVector$, alone. The value of $B^\\prime_{\\domainVariables|\\dataVariables}$ increases with incorrectness.\n",
    "\n",
    "Consistency reflects how likely different data sets are to give us different parameters. is the relative entropy (KL divergence) between the parameters of the statistical model, $\\statsProb(\\parameterVector|\\dataVariables)$, and the physical model, $\\physicsProb(\\parameterVector|\\dataVariables)$. T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absence of the Bayesian Controversy\n",
    "\n",
    "While these ideas are normally considered for thermodynamics, they can also be seen as an attempt to capture the full state of a physical system through probability. In a classical system, each of these states is the result of some deterministic (physical) relationship. \n",
    "\n",
    "The fundamentals of statistical mechanics were derived by physicists and chemists such as Maxwell, Boltzmann, Gibbs and Helmholtz. In chemistry, particle theories of matter were uncontroversial, although in physics, Boltzmann struggled throughout his life to have his fellow physicists accept these ideas and it wasn't until Einstein formulated Brownian motion with a particle model [@Einstein-brownian05], introducing stochasticity to *differential equations* and giving predictions that were verified by Perrin [@Perrin-brownian10]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermodynamic Relations\n",
    "\n",
    "The view that Boltzmann was competing with was the positivist view of physics as a universe of energy, pushed by Mach and others. \n",
    "\n",
    "The relationship between energy and particles is given through conservation of energy of the microstates. The total energy of a system is given by the expected value of those energies under probability distribution of states.\n",
    "$$\n",
    "U = \\expDist{E(\\stateVariables)}{\\mathbb{P}(\\stateVariables)}\n",
    "$$\n",
    "Note that this is related to the entropy of $\\mathbb{P}(\\stateVariables)$ as follows\n",
    "\\begin{align*}\n",
    "U = & \\expDist{E(\\stateVariables)}{\\mathbb{P}(\\stateVariables)} \\\\\n",
    "= & -\\frac{1}{\\beta}\\expDist{\\log \\mathbb{P}(\\stateVariables)}{\\mathbb{P}(\\stateVariables)} - \\frac{1}{\\beta} \\log Z  \n",
    "\\end{align*}\n",
    "where in statistical mechanics we denote \n",
    "$$\n",
    "TS = -\\frac{1}{\\beta}\\expDist{\\log\\mathbb{P}(\\stateVariables)}{\\mathbb{P}(\\stateVariables)}\n",
    "$$\n",
    "where $T$ is the temperature of the system and $S$ is the thermodynamic entropy which is given by the probabilistic entropy times Boltzmann's constant, $k_B$. For notational convenience the statistical mechanics temperature is defined as $\\beta = \\frac{1}{T k_B}$, so $Tk_B = \\frac{1}{\\beta}$. \n",
    "\n",
    "So from the probabilistic definition of expected energy, we can see that the total energy is related to the entropy as follows:\n",
    "$$\n",
    "U = TS + A\n",
    "$$\n",
    "where \n",
    "$$\n",
    "A = \\frac{1}{\\beta} \\log Z.\n",
    "$$\n",
    "In statistical mechanics this is known as the Helmholtz free energy. It is the amount of energy available for \\emph{work} or \\emph{Arbeit} in Helmholtz's original German, thus the letter $A$ to denote it. \n",
    "\n",
    "This decomposition of the joint distribution is a fundamental equation in statistical mechanics. \n",
    "\n",
    "In thermodynamics, the state variables, $\\stateVariables$ are often split into extensive and intensive variables. Extensive variables depend on the quantity of matter (total mass, volume) and intensive do not (temperature, density).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "In the world of machine learning, we are more interested in the separation between states that are observable and states that are unobservable. The statistical mechanical model we've described doesn't include measurements. Let's modify the energy function by adding in a new energy, $E(\\dataVariables | \\stateVariables)$, which represents the interaction between our measurements, $\\dataVariables$, and the state space $\\stateVariables$. \n",
    "\n",
    "Now the total energy is given by,\n",
    "$$\n",
    "U_2 = \\expDist{E(\\dataVariables | \\stateVariables)}{\\mathbb{P}(\\stateVariables| \\dataVariables)} + \\expDist{E(\\stateVariables)}{\\mathbb{P}(\\stateVariables| \\dataVariables)}\n",
    "$$\n",
    "and the Boltzmann distribution for $\\mathbb{P}(\\stateVariables | \\dataVariables)$ is given by\n",
    "$$\n",
    "\\mathbb{P}(\\stateVariables,  \\dataVariables) \\propto \\exp \\left(-\\beta E(\\stateVariables) -\\beta E(\\dataVariables | \\stateVariables)\\right)\n",
    "$$\n",
    "where the $\\dataVariables$ are values we observe in the system. \n",
    "\n",
    "This can be related to the entropy of the system given the measurements as follows, \n",
    "$$\\begin{align*}\n",
    "U_2 & = \\expDist{E(\\dataVariables |  \\stateVariables)}{\\mathbb{P}(\\stateVariables |  \\dataVariables)} + \\expDist{E(\\stateVariables)}{\\mathbb{P}(\\stateVariables | \\dataVariables)} \\\\\n",
    "& = -\\frac{1}{\\beta} \\expDist{\\log \\mathbb{P}(\\stateVariables | \\dataVariables)}{\\mathbb{P}(\\stateVariables | \\dataVariables)} - \\frac{1}{\\beta}\\log Z_2.\n",
    "\\end{align*}$$\n",
    "If we define the Helmholtz free energy to be,\n",
    "$$\n",
    "A_2 = -\\frac{1}{\\beta} \\log  Z_2\n",
    "$$\n",
    "where \n",
    "$$\n",
    "Z_2 = \\int \\exp \\left(-\\beta E(\\stateVariables) -\\beta E(\\dataVariables | \\stateVariables)\\right) \\text{d}\\stateVariables \\text{d}\\dataVariables\n",
    "$$\n",
    "then we see that our new entropy term is\n",
    "$$\n",
    "TS_2 = -\\frac{1}{\\beta} \\expDist{\\log \\mathbb{P}(\\stateVariables | \\dataVariables)}{\\mathbb{P}(\\stateVariables | \\dataVariables)}\n",
    "$$\n",
    "\n",
    "This can be seen to be related to the original entropy as follows,\n",
    "$$\n",
    "\\begin{align*}\n",
    "TS_1 = & -\\frac{1}{\\beta} \\expDist{\\log \\mathbb{P}(\\stateVariables)}{\\mathbb{P}(\\stateVariables )} \\\\\n",
    "= & -\\frac{1}{\\beta} \\int \\mathbb{P}(\\dataVariables) \\expDist{\\log \\mathbb{P}(\\stateVariables | \\dataVariables)}{\\mathbb{P}(\\stateVariables | \\dataVariables)} \\text{d}\\mathbf{y} - \\frac{1}{\\beta} \\expDist{\\log \\frac{\\mathbb{P}(\\stateVariables)}{\\mathbb{P}(\\stateVariables | \\dataVariables)}}{\\mathbb{P}(\\stateVariables, \\dataVariables)}\\\\\n",
    "= & -\\frac{1}{\\beta} \\int \\mathbb{P}(\\dataVariables) \\expDist{\\log \\mathbb{P}(\\stateVariables | \\dataVariables)}{\\mathbb{P}(\\stateVariables | \\dataVariables)} \\text{d}\\mathbf{y} + \\frac{1}{\\beta} \\expDist{\\log \\frac{\\mathbb{P}(\\stateVariables , \\dataVariables)}{\\mathbb{P}(\\stateVariables)\\mathbb{P}(\\dataVariables)}}{\\mathbb{P}(\\stateVariables, \\dataVariables)} \\\\\n",
    "= & T\\expDist{S_2}{\\mathbb{P}(\\dataVariables)}  + T I\n",
    "\\end{align*}\n",
    "$$\n",
    "where I is the mutual information between $\\dataVariables$ and $\\stateVariables$,\n",
    "$$\n",
    "I = k_B \\expDist{\\log \\frac{\\mathbb{P}(\\stateVariables , \\dataVariables)}{\\mathbb{P}(\\stateVariables)\\mathbb{P}(\\dataVariables)}}{\\mathbb{P}(\\stateVariables, \\dataVariables)}. \n",
    "$$\n",
    "\n",
    "This also allows us to look at\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\expDist{U_2}{\\mathbb{P}(\\dataVariables)} = & \\expDist{A_2}{\\mathbb{P}(\\dataVariables)} +  T\\expDist{S_2}{\\mathbb{P}(\\dataVariables)} \\\\\n",
    "= & A_2 + TS_1 - TI.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we have \n",
    "$$\n",
    "\\expDist{U_2}{\\mathbb{P}(\\dataVariables)}- U_1    = \\expDist{A_2}{\\mathbb{P}(\\dataVariables)}  - A_1 - TI\n",
    "$$\n",
    "\n",
    "If the measurements in the ML system do not disturb the marginal distribution, so that $\\mathbb{P}_2(\\stateVariables) = \\mathbb{P}_1(\\stateVariables)$ then we have \n",
    "$$\n",
    "\\expDist{U_2}{\\mathbb{P}(\\dataVariables)} = U_1 + \\expDist{E(\\dataVariables|\\stateVariables)}{\\mathbb{P}(\\dataVariables, \\stateVariables)}\n",
    "$$\n",
    "we have\n",
    "$$\n",
    "\\expDist{E(\\dataVariables|\\stateVariables)}{\\mathbb{P}(\\dataVariables, \\stateVariables)}  = \\expDist{A_2}{\\mathbb{P}(\\dataVariables)}  - A_1 - TI\n",
    "$$\n",
    "Which can must be greater or equal to zero so\n",
    "$$\n",
    "\\expDist{A_2}{\\mathbb{P}(\\dataVariables)} \\geq A_1 + TI\n",
    "$$\n",
    "so the amount of free energy in the machine learning system is at least as much as in the original system. The additional available energy is  free energy is now available as \n",
    "$$\n",
    "$$\n",
    "\n",
    "We also note that\n",
    "$$\n",
    "TS_1  \\geq TS_2 + TI\n",
    "$$\n",
    "with equality occuring when the mutual information is zero. So we are guaranteed to have reduced entropy in the machine learning system as long as the mutual information between $\\dataVariables$ and $\\stateVariables$ is greater than zero, in other words, $\\dataVariables$ and $\\stateVariables$ cannot be *independent*. \n",
    "\n",
    "Another special case would be when mutual information is maximized, which would occur if we measure every state variable. In this case we (check this) **would expect all entropy to be removed and the total energy to equal the free energy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing the Free Energy\n",
    "\n",
    "The sensible strategy for the machine learning scientist is to maximize the free energy,\n",
    "$$\n",
    "A_2 = -\\frac{1}{\\beta} \\log \\int \\exp \\left(-\\beta E(\\stateVariables) -\\beta E(\\dataVariables | \\stateVariables)\\right) \\text{d}\\stateVariables \\text{d}\\dataVariables.\n",
    "$$\n",
    "We can introduce an approximating distribution, $p(\\dataVariables, \\stateVariables)$, that represents our best understanding of how things interact. This allows us to decompose.\n",
    "$$\n",
    "\\begin{align*}\n",
    "-\\frac{1}{\\beta} \\log \\int \\exp \\left(-\\beta E(\\stateVariables) -\\beta E(\\dataVariables | \\stateVariables)\\right) \\text{d}\\stateVariables \\text{d}\\dataVariables = & -\\frac{1}{\\beta} \\int p(\\dataVariables, \\stateVariables) \\log \\frac{\\exp\\left(-\\beta E(\\stateVariables) -\\beta E(\\dataVariables | \\stateVariables)\\right)}{p(\\dataVariables, \\stateVariables)}\\text{d}\\stateVariables \\text{d}\\dataVariables \\\\\n",
    "& +\\frac{1}{\\beta} \\int p(\\dataVariables, \\stateVariables) \\log \\frac{\\mathbb{P}(\\dataVariables, \\stateVariables)}{p(\\dataVariables, \\stateVariables)}\\text{d}\\stateVariables \\text{d}\\dataVariables. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "or for a given observation of $\\dataVariables$ we have,\n",
    "$$\n",
    "\\begin{align*}\n",
    "-\\frac{1}{\\beta} \\log \\int \\exp \\left(-\\beta E(\\stateVariables) -\\beta E(\\dataVariables | \\stateVariables)\\right) \\text{d}\\stateVariables  = & -\\frac{1}{\\beta} \\int p(\\stateVariables| \\dataVariables) \\log \\frac{\\exp\\left(-\\beta E(\\stateVariables) -\\beta E(\\dataVariables | \\stateVariables)\\right)}{p( \\stateVariables| \\dataVariables)}\\text{d}\\stateVariables  \\\\\n",
    "& +\\frac{1}{\\beta} \\int p(\\stateVariables |\\dataVariables) \\log \\frac{\\mathbb{P}(\\stateVariables| \\dataVariables)}{p(\\stateVariables | \\dataVariables)}\\text{d}\\stateVariables. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "A_2 = & \\expDist{E(\\stateVariables) +E(\\dataVariables | \\stateVariables)}{p(\\dataVariables, \\stateVariables)} \\\\ \n",
    "& - \\frac{1}{\\beta} \\expDist{\\log p(\\dataVariables, \\stateVariables)}{p(\\dataVariables, \\stateVariables)} \\\\\n",
    "& +\\frac{1}{\\beta} \\int p(\\dataVariables, \\stateVariables) \\log \\frac{\\mathbb{P}(\\dataVariables, \\stateVariables)}{p(\\dataVariables, \\stateVariables)}\\text{d}\\stateVariables \\text{d}\\dataVariables.\n",
    "\\end{align*}\n",
    "$$\n",
    "reordering as\n",
    "$$\n",
    "\\begin{align*}\n",
    "A_2 + \\frac{1}{\\beta} \\expDist{\\log \\frac{p(\\dataVariables, \\stateVariables)}{\\mathbb{P}(\\dataVariables, \\stateVariables)}}{p(\\dataVariables, \\stateVariables) } = & \\expDist{E(\\stateVariables) +E(\\dataVariables | \\stateVariables)}{p(\\dataVariables, \\stateVariables)} \n",
    "+ \\frac{1}{\\beta} \\expDist{\\log p(\\dataVariables, \\stateVariables)}{p(\\dataVariables, \\stateVariables)}.\n",
    "\\end{align*}\n",
    "$$\n",
    "We recognise that the left hand side is the free energy plus a Kullback-Leibler (KL) divergence between our approximating distribution, $p(\\dataVariables, \\stateVariables)$ and the true distribution $\\mathbb{P}(\\dataVariables, \\stateVariables)$. We introduce an apparent free energy,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{A}_2 = & A_2 + \\frac{1}{\\beta} \\expDist{\\log \\frac{p(\\dataVariables, \\stateVariables)}{\\mathbb{P}(\\dataVariables, \\stateVariables)}}{p(\\dataVariables, \\stateVariables)} \\\\\n",
    "= & A_2 + T \\text{KL}\\left(p(\\dataVariables, \\stateVariables) || \\mathbb{P}(\\dataVariables, \\stateVariables)\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "which, because the KL divergence is greater than or equal to zero, shows that the apparent free energy is an *upper bound* on the actual free energy. The tightness of the bound improves as our approximation, $p(\\dataVariables, \\stateVariables)$ approaches the truth $\\mathbb{P}(\\dataVariables, \\stateVariables)$. \n",
    "\n",
    "The other two terms can be seen to be equivalent to the total energy, $U_2$, and the entropy, $S_2$. But again under the assumed distribution, $p(\\dataVariables, \\stateVariables)$, so we have\n",
    "$$\n",
    "\\hat{U}_2 = \\expDist{E(\\stateVariables) +E(\\dataVariables | \\stateVariables)}{p(\\dataVariables, \\stateVariables)}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "T\\hat{S}_2 = - \\frac{1}{\\beta} \\expDist{\\log p(\\dataVariables, \\stateVariables)}{p(\\dataVariables, \\stateVariables)}\n",
    "$$\n",
    "This allows us to write down a new assumed free energy relationship.\n",
    "$$\n",
    "\\hat{A}_2 = \\hat{U}_2 - T\\hat{S}_2.\n",
    "$$\n",
    "This is the world that, as modellers, we play with. The total energy term and the entropy term are now given by our *assumed* probability distribution for the world, $p(\\dataVariables, \\stateVariables)$. Or in other words by the model we use. That model is an approximation to the truth, $\\mathbb{P}(\\dataVariables, \\stateVariables)$. However, rather insiduously, the worse our approximation to the truth, the greater the apparent free energy appears to be. Because the apparent free energy is an upper bound on the true free energy, we have to be very careful about model fitting,\n",
    "$$\n",
    "A_2 = \\hat{A}_2 - T \\text{KL}\\left(p(\\dataVariables, \\stateVariables) || \\mathbb{P}(\\dataVariables, \\stateVariables)\\right).\n",
    "$$\n",
    "Normally we would be looking to maximize the free energy, which minimizes the entropy. But here we have to be careful about direct maximization, because a high *apparent* free energy can be achieved with a poor model.\n",
    "\n",
    "Note also, that traditional approaches to probabilistic model fitting are often justified by minimization of the KL divergence. But that KL divergence is the *other way around*. Classical maximum likelihoood minimizes $T \\text{KL}\\left(\\mathbb{P}(\\dataVariables, \\stateVariables) || p(\\dataVariables, \\stateVariables)\\right)$. I.e. expectations are taken under the true distribution $\\mathbb{P}(\\dataVariables, \\stateVariables)$.\n",
    "\n",
    "So far, everything we've written about concerns *equilibrium thermodynamics*. The above laws apply once we have stationary distributions. In theory, that might take infinite time to occur. If we only consider equilibrium thermodynamics, we loose the element of time. Time is just as important as free energy as a resource to preserve. When building an intelligent system we are very often faced with a time horizon. Algorithms that require us to compute for many billions of years to get their answers are not of much use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "-\\frac{1}{\\beta} \\log \\int \\exp \\left(-\\beta E(\\stateVariables) -\\beta E(\\dataVariables | \\stateVariables)\\right) \\text{d}\\stateVariables \\text{d}\\dataVariables = & -\\frac{1}{\\beta} \\int p(\\dataVariables)p(\\stateVariables | \\dataVariables) \\log \\frac{\\exp\\left(-\\beta E(\\stateVariables) -\\beta E(\\dataVariables | \\stateVariables)\\right)}{p(\\stateVariables|\\dataVariables) }\\text{d}\\stateVariables \\text{d}\\dataVariables  \\\\\n",
    "& +\\frac{1}{\\beta} \\int p(\\dataVariables) p(\\stateVariables|\\dataVariables) \\log \\frac{\\mathbb{P}( \\stateVariables| \\dataVariables)}{p( \\stateVariables | \\dataVariables)}\\text{d}\\stateVariables \\text{d}\\dataVariables + \\frac{1}{\\beta} \\int p(\\dataVariables)  \\log  \\mathbb{P}(  \\dataVariables) \\text{d}\\dataVariables.\\\\\n",
    "= & -\\frac{1}{\\beta} \\int p(\\dataVariables)p(\\stateVariables | \\dataVariables) \\log \\frac{\\exp\\left( -\\beta E(\\dataVariables | \\stateVariables)\\right)}{p(\\stateVariables|\\dataVariables) }\\text{d}\\stateVariables \\text{d}\\dataVariables + \\int p(\\dataVariables) p(\\stateVariables|\\dataVariables) E(\\stateVariables) \\text{d}\\stateVariables \\\\\n",
    "& +\\frac{1}{\\beta} \\int p(\\dataVariables) p(\\stateVariables|\\dataVariables) \\log \\frac{\\mathbb{P}( \\stateVariables| \\dataVariables)}{p( \\stateVariables | \\dataVariables)}\\text{d}\\stateVariables \\text{d}\\dataVariables + \\frac{1}{\\beta} \\int p(\\dataVariables)  \\log  \\mathbb{P}(  \\dataVariables) \\text{d}\\dataVariables.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing the Available Energy\n",
    "\n",
    "The apparent available energy is given by\n",
    "$$\n",
    "\\hat{A}_2 =  \\expDist{E(\\stateVariables) +E(\\dataVariables | \\stateVariables)}{p(\\dataVariables, \\stateVariables)} + \\frac{1}{\\beta} \\expDist{\\log p(\\dataVariables, \\stateVariables)}{p(\\dataVariables, \\stateVariables)}\n",
    "$$\n",
    "which differs from the true available energy by a KL divergence that represents the *physical plausibility* of the model, $T \\text{KL}\\left(p(\\dataVariables, \\stateVariables) || \\mathbb{P}(\\dataVariables, \\stateVariables)\\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximising the Apparent Total Energy\n",
    "\n",
    "Our first idea could be to maximise the apparent total energy, $\\hat{U}_2$. This would require us to have an estimate of what that total energy is so we would have, \n",
    "$$\n",
    "\\hat{E}(\\dataVariables, \\stateVariables)  \\approx E(\\stateVariables) + E(\\dataVariables | \\stateVariables).\n",
    "$$\n",
    "Or more precisely an estimate of the expectations under our joint distribution.\n",
    "$$\n",
    "\\expDist{\\hat{E}(\\dataVariables, \\stateVariables)}{p(\\dataVariables, \\stateVariables)} \\approx \\expDist{E(\\stateVariables) + E(\\dataVariables | \\stateVariables)}{p(\\dataVariables, \\stateVariables)}.\n",
    "$$\n",
    "Maximising the expected total energy could be done through proxies which relate those energies to actual costs in the real world, so we can see the approximation $\\expDist{\\hat{E}_2(\\dataVariables, \\stateVariables)}{p(\\dataVariables, \\stateVariables)}$ as an objective to be maximised, or its negative as cost to be minimized. \n",
    "\n",
    "Minimising expected costs is reminiscient of operational research, where the (often montetary) costs of a process are enumerated, and optimization algorithms are used to find configurations that minimize expected cost. Optimisng the total energy in this way is optimal when there is no uncertainty, i.e. when \n",
    "$$\n",
    "\\expDist{\\hat{E}(\\dataVariables, \\stateVariables)}{\\mathbb{P}(\\dataVariables, \\stateVariables)} =  \\hat{E}\\left(\\expDist{\\stateVariables}{\\mathbb{P}(\\dataVariables)}, \\expDist{\\dataVariables}{\\mathbb{P}(\\dataVariables)}\\right) .\n",
    "$$\n",
    "Because in this case the entropy term can be ignored and the apparent available energy is equal to the total energy. In this case, our estimate of the apparent available energy would differ from the true available energy by the following residual,\n",
    "$$\n",
    "E(\\langle\\stateVariables\\rangle) + E(\\langle\\dataVariables\\rangle | \\langle\\stateVariables\\rangle) - \\hat{E}(\\langle\\dataVariables\\rangle | \\langle\\stateVariables\\rangle).\n",
    "$$\n",
    "But for our focus, when there is uncertainty in the system, we need to consider the entropy term, $T\\hat{S}_2$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy Distribution\n",
    "\n",
    "Maximising the apparent total energy is the most optimistic we can be, we assume no uncertainty. The most pessimistic we can be would be to consider the situation where the entropy, $\\hat{S}_2$ is *maximized*. We can see the maximum apparent total energy as an optimistic approach, maximising is the other extreme.\n",
    "\n",
    "The maximum entropy principle [@Jaynes-] allows us to find a form for $p(\\dataVariables, \\stateVariables)$ that maximises $T\\hat{S}_2$ while ensuring constraints on the distribution moments match some known value. Here the idea constraint on moments would be,\n",
    "$$\n",
    "\\expDist{E(\\dataVariables | \\stateVariables)}{p(\\dataVariables, \\stateVariables)} = \\expDist{E(\\dataVariables | \\stateVariables)}{\\mathbb{P}(\\dataVariables, \\stateVariables)}.\n",
    "$$\n",
    "This constraint would recover $\\mathbb{P}(\\dataVariables, \\stateVariables)$. This would reflect a world where we had full knowledge of the physics. In practice, as we sugged for the maximisaiton of the apparent total energy, we may \n",
    "$$\n",
    "p(\\dataVariables, \\stateVariables) \\propto \\exp(\\lambda \\hat{E}(\\dataVariables, \\stateVariables))\n",
    "$$\n",
    "with a partition function of the form\n",
    "$$\n",
    "\\hat{Z}_2 = \\int \\exp(\\lambda \\hat{E}(\\dataVariables , \\stateVariables)) \\text{d} \\dataVariables \\text{d} \\stateVariables.\n",
    "$$\n",
    "which implies that,\n",
    "$$\n",
    "\\hat{A}_2 = \\expDist{E(\\stateVariables) +E(\\dataVariables | \\stateVariables) - \\frac{\\lambda}{\\beta}\\hat{E}(\\dataVariables, \\stateVariables)}{p(\\dataVariables, \\stateVariables)} - \\frac{1}{\\beta} \\log \\hat{Z}_2.\n",
    "$$\n",
    "Which reflects how close our model is to the physical reality. We can substitute, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "with the different modelling steps we have taken so far, we can relate the original total energy of the system to our new system as follows,\n",
    "$$\n",
    "\\hat{A}_2 = U_1 + T I(\\dataVariables, \\stateVariables) + T\\text{KL}\\left(p(\\dataVariables, \\stateVariables) || \\mathbb{P}(\\dataVariables, \\stateVariables)\\right) - TS_1 - TS_2 - T\\hat{S}_2.\n",
    "$$\n",
    "where we are gaining energy from two sources, firstly through the information gain from $\\dataVariables$. This is genuine energy gain. More problematically is the information gain from the KL divergence. This KL divergence represents the physical plausibility of our model, $p(\\dataVariables, \\stateVariables)$. This is an illusory gain, because the less physical our model, the more information we seem to gain. We can immediately see how easy it is to fool ourselves by building models that don't correspond to the physical reality of our world. \n",
    "\n",
    "We are loosing energy due to uncertainty, first of all we loose energy that corresponds to our ignorance about the phase space, $\\stateSpace$. Then we loose energy that conforms to our ignorance \n",
    "\n",
    "We can also see the difference between $\\hat{A}_2$\n",
    "Laplace's demon can be seen as relating the total energy to the available enrgy. , these two terms relate to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Approximations\n",
    "\n",
    "Firstly, let's deal with computational approximations. \n",
    "\n",
    "Introduce an auxilliary variable to the system, $\\parameterVector$. Assume we can decompose,\n",
    "$$\n",
    "p(\\stateVariables, \\dataVariables) = \\int p(\\dataVariables| p(\\stateVariables | \\parameterVector) p(\\parameterVector) \\text{d} \\parameterVector\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{A}_2 = \\expDist{E(\\dataVariables , \\stateVariables) - \\frac{\\lambda}{\\beta}\\hat{E}(\\dataVariables, \\stateVariables)}{p(\\stateVariables | \\dataVariables)} - \\frac{1}{\\beta} \\log \\hat{Z}_2.\n",
    "$$\n",
    "$$\\begin{align*}\n",
    "\\hat{Z}_2 = & \\int \\exp\\left(\\lambda\\hat{E}(\\dataVariables, \\stateVariables)\\right) \\text{d}\\stateVariables\\\\\n",
    "= & \\int q(\\stateVariables, \\parameterVector) \\log \\frac{\\exp\\left(\\lambda\\hat{E}(\\dataVariables, \\stateVariables)\\right)p(\\stateVariables|\\parameterVector) p(\\parameterVector)}{q(\\stateVariables, \\parameterVector)} \\text{d}\\stateVariables \\text{d}\\parameterVector+ \\int q(\\stateVariables, \\parameterVector) \\log \\frac{q(\\stateVariables, \\parameterVector)p(\\dataVariables)}{p(\\dataVariables|\\stateVariables)p(\\stateVariables | \\parameterVector)p(\\parameterVector)} \\text{d}\\stateVariables \\text{d}\\parameterVector\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "\\hat{Z}_2 = & \\int \\exp\\left(\\lambda\\hat{E}(\\dataVariables, \\stateVariables)\\right) \\text{d}\\stateVariables\\\\\n",
    "= & \\int q(\\parameterVector) \\log \\frac{\\exp\\left(\\lambda\\hat{E}(\\dataVariables, \\expDist{\\stateVariables}{p(\\stateVariables|\\parameterVector)})\\right)p(\\parameterVector)}{q(\\parameterVector)} \\text{d}\\stateVariables \\text{d}\\parameterVector+ \\int q(\\stateVariables, \\parameterVector) \\log \\frac{q(\\stateVariables, \\parameterVector)p(\\dataVariables)}{p(\\dataVariables|\\stateVariables)p(\\stateVariables | \\parameterVector)p(\\parameterVector)} \\text{d}\\stateVariables \\text{d}\\parameterVector\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Equilibrium Thermodynamics\n",
    "\n",
    "\n",
    "### Crook's Fluctuation Theorem\n",
    "\n",
    "Crook's fluctuation theorem [@Crooks-fluctuation99] relates the work done on a system to the free energy difference between the final and initial state even when the system has not reached equilibrium.\n",
    "\n",
    "is an equation in statistical mechanics that relates the work done on a system during a non-equilibrium transformation to the free energy difference between the final and the initial state of the transformation. During the non-equilibrium transformation the system is at constant volume and in contact with a heat reservoir. The CFT is named after the chemist Gavin E. Crooks (then at University of California) \\cite{}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
